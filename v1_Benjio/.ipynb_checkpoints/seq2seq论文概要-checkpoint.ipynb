{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 今天主要看了下论文，主要讲了一下内容："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### arxiv.org/abs/1409.0473\n",
    "<br>\n",
    "主要是原始nmt的一种改进，原来的nmt不管输入的句子的长短都通过encode生成固定长度的context c。作者认为这是限制nmt发展的关键。在这篇论文中，作者提出了几个改进的地方：\n",
    "1. encoder使用BiRnn\n",
    "2. 加入attention机制。他的想法是在生成y_i的时候对应的环境参数c应该更多的考虑第i个单词，及其周围单词的信息。实现方法是使它的值受encoder的输出，decoder前一步的内部参数，以及decoder前一步的输出的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### arxiv.org/abs/1412.2007\n",
    "<br>\n",
    "这篇论文主要解决了，当单词集很大的时候，导致的模型复杂度线性上升的问题。通过这篇论文中提出的方法，可以使复杂度固定为一个常数。另外，这篇论文还谈及了NMT相关的其他的一些内容。\n",
    "1. NMT的主要优点有：专业语言知识需求小，可统一调整训练，内存需求小。\n",
    "2. NMT的主要限制就是刚才提到的词汇量问题。\n",
    "3. 一种naive的实现：取最频繁的k个单词生成词汇表，其他没在单词表中的单词用UNK：unknown word代替。这种方法的UNK的数量多的时候，其结果有点惨不忍睹\n",
    "4. 简单的介绍了一下birann encoder和attention机制，说明为什么词汇量大的时候，会提高复杂度（必须计算出decoder输出与所有单词的点乘，来确定最终单词。所以最后层W是一个[n, m]矩阵n表示单词量的大小，m表示embedding的长度）\n",
    "5. 提出优化的两个方向：model-specific和translate-specific。model-specific的主要方法有概率拟合和词汇聚类。\n",
    "6. 提出优化方法，并进行解释。通过梯度推导计算发现，最终输出单词的log概率的梯度有正负两个项确定，其中负项为decoder输出的期望。因此，这个方法的中心思想就是，通过先预设置的分布，取样模拟前面提到的期望。因为取样为完整词汇的子集，所以可以优先解决复杂度问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体实现只能翻看论文了。不知道markdown怎么编辑公式，等学了再说吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
