{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般构建一个比较完整的模型，都要分三个类进行编码，分别是数据处理类(data_utils)，运行类(translate)，和模型类(seq2seq_model)。三个类分别至少必须完成各自最基本的任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import os\n",
    "import gzip\n",
    "import urllib\n",
    "import time\n",
    "import logging\n",
    "import sys #用于本地输入测试\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from six.moves import xrange #返回xrange类，相较于range，适用于长队列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理类，必需实现以下方法：\n",
    "- 获取数据，如果有必要则通过urllib或scrapy，从网上抓取\n",
    "- 处理数据，分类，分行，去噪等工作\n",
    "- 准备数据，根据模型需要，准备所有数据，确保打开就能够使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_download(filename, url):\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading %s to %s\" %(url, filename))\n",
    "        filename, _ = urllib.request.urlretrieve(url, filename)\n",
    "        statinfo = os.stat(filename)\n",
    "        print(\"Successfully downloaded\", filename, statinfo.st_size, \"bytes\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.statmt.org/wmt10/training-giga-fren.tar to training-giga-fren.tar\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-659dd33c63a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://www.statmt.org/wmt10/training-giga-fren.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training-giga-fren.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-b0f6a6fb121c>\u001b[0m in \u001b[0;36mdata_download\u001b[0;34m(filename, url)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading %s to %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mstatinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Successfully downloaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url = \"http://www.statmt.org/wmt10/training-giga-fren.tar\"\n",
    "filename = \"training-giga-fren.tar\"\n",
    "data_download(filename, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行类必需处理以下任务：\n",
    "- 读取数据\n",
    "- 建立模型\n",
    "- 训练模型：包括训练，记录和读取checkpoint\n",
    "- 测试模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置所有需要预设的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --learning_rate: conflicting option string: --learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a8eb5c9df80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate_decay_factor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate decaus by this moch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_gradient_norm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Clip gradients to this norm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Batch size to use during the training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size of each model layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mDEFINE_float\u001b[0;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0m_define_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m _allowed_symbols = [\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m_define_helper\u001b[0;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                               type=flagtype)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1709\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1711\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3-tf/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1508\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1510\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --learning_rate: conflicting option string: --learning_rate"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99, \"Learning rate decaus by this moch\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0, \"Clip gradients to this norm\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size to use during the training\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model\")\n",
    "tf.app.flags.DEFINE_integer(\"from_vocab_size\", 40000, \"English vocabulary size\")\n",
    "tf.app.flags.DEFINE_integer(\"to_vocab_size\", 40000, \"French vocabulary size\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"./tmp\", \"Data directory\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"./tmp\", \"Training directory\")\n",
    "tf.app.flags.DEFINE_string(\"from_train_data\", None, \"Training data\")\n",
    "tf.app.flags.DEFINE_string(\"to_train_data\", None, \"Training data\")\n",
    "tf.app.flags.DEFINE_string(\"from_dev_data\", None, \"Training data\")\n",
    "tf.app.flags.DEFINE_string(\"to_dev_data\", None, \"Trraining data\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0, \"Limit on the size of training data(0: no limit\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200, \"How many training steps to do per checkpoint\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False, \"Set to True for interactive decoding\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False, \"Run a self-test if this is set to True\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_fp16\", False, \"Train using fp16 instead of fp32\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为重复运行，所以会出现上面重名的错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "# 这个适用于分类句子的长度，使得相近长度的句子能够被分配到相应的分组里面\n",
    "# 因为这个算法又补足长度的说法（pad），所以这么做可以提高效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(source_path, target_path, max_size = None):\n",
    "    data_set = [[] for _ in _buckets]\n",
    "    with tf.gfile.GFile(source_path, mode = 'r') as source_file:\n",
    "        with tf.gfile.GFile(target_path, mode = 'r') as target_file:\n",
    "            source, target = source_file.readline(), target_file.readline()\n",
    "            counter = 0\n",
    "            while source and target and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                if counter % 100000 == 0:\n",
    "                    print(\" reading data line %d\" %counter)\n",
    "                    sys.stdout.flush()\n",
    "                # 叫id是因为文本文件在预处理中已经都被转化为对应的数值了\n",
    "                source_ids = [int(x) for x in source.split()]\n",
    "                target_ids = [int(x) for x in target.split()]\n",
    "                target_ids.append(EOS_ID) # 加了个结尾标志\n",
    "                for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "                    if len(surce_ids) < source_size and len(target_ids) < target_size:\n",
    "                        # 选择合适大小的bucket来装载数据\n",
    "                        data_set[bucket_id].append([source_ids, target_ids])\n",
    "                        break\n",
    "                source, target = source_file.readline(), target_file.readline()\n",
    "        return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mode(session, forward_only):\n",
    "    # 构建模型，如果存在checkpoint的话，加载checkpoint\n",
    "    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "    model = Seq2SeqModel(FLAGS.from_vocab_size,\n",
    "                        FLAGS.to_vocab_size,\n",
    "                        _buckets,\n",
    "                        FALGS.size,\n",
    "                        FLAGS.num_layers,\n",
    "                        FLAGS.max_gradient_norm,\n",
    "                        FLAGS.batch_size,\n",
    "                        FLAGS.learning_rate,\n",
    "                        FLAGS.learning_rate_decay_factor,\n",
    "                        forward_only = forward_only,\n",
    "                        dtype = dtype)\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "    if ckpt and tf.train.checkpoint_exist(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" %ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    from_train = None\n",
    "    to_train = None\n",
    "    from_dev = None\n",
    "    to_dev = None\n",
    "    if FLAGS.from_train_data and FLAGS.to_train_data:\n",
    "        from_train_data = FLAGS.from_train_data\n",
    "        to_train_data = FLAGS.to_train_data\n",
    "        from_dev_data = from_train_data\n",
    "        to_dev_data = to_train_data\n",
    "        if FLAGS.from_dev_data and FLAGS.to_dev_data:\n",
    "            from_dev_data =FLAGS.from_dev_data\n",
    "            to_dev_data = FLAGS.to_dev_data\n",
    "        from_train, to_train, from_dev, to_dev, _, _ = prepare_data(FLAGS.data_dir,\n",
    "                                                                   from_train_data,\n",
    "                                                                   to_train_data,\n",
    "                                                                   from_dev_data,\n",
    "                                                                   to_dev_data,\n",
    "                                                                   FLAGS.from_vocab_size,\n",
    "                                                                   FLAGS.to_vocab_size)\n",
    "    else:\n",
    "        print(\"Preparing WMT data in %s\" %FLAGS.data_dir)\n",
    "        from_train, to_train, from_dev, to_dev, _, _ = prepare_wmt_data(FLAGS.data_dir,\n",
    "                                                                       FLAGS.from_vocab_size,\n",
    "                                                                       FLAGS.to_vocab_size)\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Creating %d layers of %d units.\" %(FLAGS.num_layers, FLAGS.size))\n",
    "        model = create_nodel(sess, False)\n",
    "        \n",
    "        print(\"Reading development and training data(limit: %d)\" %FLAGS.max_train_data_size)\n",
    "        dev_set = read_data(from_dev, to_dev)\n",
    "        train_set = read_data(from_train, to_train, FLAGS.max_train_data_size)\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        \n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i+1])/train_total_size for i in xrange(len(train_bucket_sizes))]\n",
    "        \n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        while True:\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                            if train_buckets_scale[i] > random_number_01])\n",
    "            start_time = time.time()\n",
    "            encoder_intpus, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "            step_time += (time.time() - start_time)/FLAGS.steps_per_checkpoint\n",
    "            loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "            current_step += 1\n",
    "            \n",
    "            if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "                print(\"global step %d learning rate %.4f step-time %.2f perplexity\" \n",
    "                     \"%.2f\" %(model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n",
    "                \n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                \n",
    "                checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n",
    "                model.saver.save(sess, checkpoint_path, global_step = model.global_step)\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                \n",
    "                # run evals on development set and print their perplexity\n",
    "                for bucket_id in xrange(len(_buckets)):\n",
    "                    if len(dev_set[bucket_di]) == 0:\n",
    "                        print(\" eval: empty bucket %d\" %(bucket_id))\n",
    "                        continue\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "                    eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "                    print(\" eval: bucket %d perplexity %.2f\" %(bucket_id, eval_ppx))\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型类应该至少完成下面的任务：\n",
    "- 构建模型：包括定义cell和拓扑结构\n",
    "- 设置饲料\n",
    "- 提供步训练函数并返回loss和accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下三份paper，详细的描述了这个模型，他们分别是：\n",
    "- http://arxiv.org/abs/1412.7449 描述了模型的基本架构\n",
    "- http://arxiv.org/abs/1409.0473 描述了单层，双向编码的时候的模型架构\n",
    "- http://arxiv.org/abs/1412.2007 第三章具体描述了sampled softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self,\n",
    "                source_vocab_size,\n",
    "                target_vocab_size,\n",
    "                buckets,\n",
    "                size,\n",
    "                num_layers,\n",
    "                max_gradient_norm,\n",
    "                batch_size,\n",
    "                learning_rate,\n",
    "                learning_rate_decay_factor,\n",
    "                use_lstm= False,\n",
    "                num_samples = 512,\n",
    "                forward_only = False,\n",
    "                dtype = tf.float32):\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.buckets = buckets\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate),\n",
    "                                        trainable = False,\n",
    "                                        dtype = dtype)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        # needs a output projection for sampled softmax\n",
    "        output_projection = None\n",
    "        softmax_loss_function = None\n",
    "        # 如果sample集大于单词集，那么就木有意义了\n",
    "        if num_samples > 0 and num_samples < self.target_vocab_size:\n",
    "            w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype = dtype)\n",
    "            w = tf.transpose(w_t)\n",
    "            b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype = dtype)\n",
    "            output_projection = (w, b)\n",
    "            def sample_loss(labels, logits):\n",
    "                # 第一步不知道是要干什么？？\n",
    "                labels = tf.reshape(labels, [-1, 1]) \n",
    "                # We need to compute the sampled_softmax_loss using 32bit floats to avoid numerical instabilities??\n",
    "                local_w_t = tf.cast(w_t, tf.float32)\n",
    "                local_b = tf.cast(b, tf.float32)\n",
    "                local_inputs = tf.cast(logits, tf.float32)\n",
    "                return tf.cast(tf.nn.sampled_softmax_loss(weights = local_w_t,\n",
    "                                                         biases = local_b,\n",
    "                                                         labels = labels,\n",
    "                                                         inputs = local_inputs,\n",
    "                                                         num_sampled = num_samples,\n",
    "                                                         num_classes = self_target_vocab_size), dtype)\n",
    "            softmax_loss_function = sampled_loss\n",
    "            # 生成节点: 所以生成的是一个layer而不是一个节点了？？\n",
    "        def single_cell():\n",
    "            return tf.contrib.rnn.GRUCell(size)\n",
    "        if use_lstm:\n",
    "            def single_cell():\n",
    "                return tf.contrib.rnn.BasicLSTMCell(size)\n",
    "        cell = single_cell()\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n",
    "        # 生成模型\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(encoder_inputs,\n",
    "                                                                        decoder_inputs,\n",
    "                                                                        cell,\n",
    "                                                                        num_encoder_symblos = source_vocab_size,\n",
    "                                                                        num_decoder_symbles = target_vocab_size,\n",
    "                                                                        embedding_size = size,\n",
    "                                                                        output_projection = output_projection,\n",
    "                                                                        feed_previous = do_decode, \n",
    "                                                                        dtype = dtype)\n",
    "        # 设置饲料\n",
    "        self.encoder_input = []\n",
    "        self.decoder_input = []\n",
    "        self.target_weights = []\n",
    "        for i in xrange(buckets[-1][0]):\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32,\n",
    "                                                     shape = [None],\n",
    "                                                     name = \"encoder{0}\".format(i)))\n",
    "        for i in xrange(buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32,\n",
    "                                                     shape = [None],\n",
    "                                                     name = \"decoder{0}\".format(i)))\n",
    "            self.target_weight.append(tf.placeholder(dtype,\n",
    "                                                    shape = [None],\n",
    "                                                    name = \"weight{0}\".format(i)))\n",
    "        # 注意： our target are decoder input shifted by one\n",
    "        targets = [self.decoder_input[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "        # 设置输出和损失\n",
    "        if forward_only:\n",
    "            self.outputs, self.loss = tf.contrib.legacy_seq2seq.model_with_buckets(self.encoder_inputs,\n",
    "                                                                                  self.decoder_inputs,\n",
    "                                                                                  targets,\n",
    "                                                                                  self.target_weights,\n",
    "                                                                                  buckets,\n",
    "                                                                                  lambda x, y: seq_seq_f(x, y, True),\n",
    "                                                                                  softmax_loss_function = softmax_loss_function)\n",
    "            # If we use output projection, we need to project outpus for decoding\n",
    "            if output_projection is not None:\n",
    "                for b in xrange(len(buckets)):\n",
    "                    self.outputs[b] = [\n",
    "                        tf.matmul(output. output_projection[0]) + output_projection[1]\n",
    "                        for output in self.outputs[b]\n",
    "                    ]\n",
    "        else:\n",
    "            self.output. self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(self.encoder_inputs,\n",
    "                                                                                   self.decoder_inputs,\n",
    "                                                                                   targets,\n",
    "                                                                                   self.target_weights,\n",
    "                                                                                   buckets,\n",
    "                                                                                   lambda x, y: seq_seq_f(x, y, False),\n",
    "                                                                                   softmax_loss_function = softmax_loss_function)\n",
    "        # 设置更新和训练\n",
    "        params = tf.trainable_variables()\n",
    "        # 这部分没有看懂？？？\n",
    "        if not forward_only:\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            for b in xrange(len(buckets)):\n",
    "                gradients = tf.gradients(self.loss[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.update.append(opt.apply_gradients(zip(clipped_gtadients, params), global_step = self.global_step))\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only):\n",
    "        # 检测大小\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        if len(encoder_inputs) != encoder_size:\n",
    "            raise ValueError(\"Encoder length must be equal to the one in bucket\")\n",
    "        if len(encoder_inputs) != decoder_size:\n",
    "            raise ValueError(\"Decoder lenghth must be equal to the one in bucket\")\n",
    "        if len(target_weights) != decoder_size:\n",
    "            raise ValueError(\"Weight length must be equal to the one in bucket\")\n",
    "        \n",
    "        inpur_feed = {}\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype = np.int32)\n",
    "        \n",
    "        if not forward_only:\n",
    "            output_feed = [self.updates[bucket_id],\n",
    "                          self.gradient_norms[bucket_id],\n",
    "                          self.losses[bucket_id]]\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]]\n",
    "            for l in xrange(decoder_size):\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "        \n",
    "        outputs = session.run(output_feed, input_feed) # 为什么没说run什么东西？？\n",
    "        if not forward_only:\n",
    "            return outputs[1], outputs[2], None # Gradient norm, loss, no outputs\n",
    "        else:\n",
    "            return None, outputs[0], outputs[1:] # No gradient norm, loss, outputs\n",
    "    \n",
    "    def get_batch(self, data, bucket_id):\n",
    "        decoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        encoder_inputs, decoder_inputs = [], []\n",
    "        for _ in xrange(self.batch_size):\n",
    "            encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "            \n",
    "            # Encoder inputs are padded and then reversed\n",
    "            encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
    "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "            # Decoder inputs get an extra \"GO\" symbol, and are padded then\n",
    "            decoder_pad_size = decoder_size - len(decoder_input) -1\n",
    "            decoder_inputs.append([GO_ID] + decoder_input + [PAD_ID] * decoder_pad_size)\n",
    "            # Now we create the batch-major vectors from the data selected above\n",
    "            batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "            # Batch encoder inputs are just re-indexed encoder_inputs\n",
    "            for length_idx in xrange(encoder_size):\n",
    "                batch_encoder_inputs.append(np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                                                     for batch_idx in xrange(self.batch_size)], dtype = np.int32))\n",
    "            # Batch decoder inputs are re-indexed decoder_inputs, we create a weights\n",
    "            for length_idx in xrange(decoder_size):\n",
    "                batch_decoder_inputs.append(np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                                                     for batch_idx in xrange(self.batch_size)], dtype = np.int32))\n",
    "                # Create targget_weights to be 0 for targets that are padding \n",
    "                batch_weight = np.ones(self.batch_size, dtype = np.float32)\n",
    "                for batch_idx in xrange(self.batch_size):\n",
    "                    # The corresponding target is docoder_input shifted by 1 forward\n",
    "                    # we set weight to 0 if the corresponding target is a PAD symbol\n",
    "                    if length_idx < decoder_size -1:\n",
    "                        target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                    if length_idx == decoder_size -1 or target == PAD_ID:\n",
    "                        batch_weight[batch_idx] = 0.0\n",
    "                    batch_weights.append(batch_weight)\n",
    "                return batch_encoder_inputs, batch_decoder_inputs, batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
